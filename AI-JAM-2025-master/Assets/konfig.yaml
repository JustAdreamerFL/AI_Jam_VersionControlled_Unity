behaviors:
  Marienka:
    trainer_type: ppo

    hyperparameters:
      batch_size: 4096
      buffer_size: 40960
      learning_rate: 2.5e-4
      beta: 0.0006
      epsilon: 0.15
      lambd: 0.97
      num_epoch: 4
      learning_rate_schedule: linear

    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3

    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0

    max_steps: 1.5e7     
    time_horizon: 128
    summary_freq: 1000
    checkpoint_interval: 1000000
    threaded: false

  Janko:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 2
      vis_encode_type: simple
     
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
   
    max_steps: 1.5e7 
    time_horizon: 64
    summary_freq: 1000
    checkpoint_interval: 500000
    threaded: false

  GregorKameraBlizko:
      trainer_type: ppo

      hyperparameters:
        batch_size: 2048
        buffer_size: 40960
        learning_rate: 2.5e-4
        beta: 0.0005
        epsilon: 0.15
        lambd: 0.97
        num_epoch: 4
        learning_rate_schedule: linear

      network_settings:
        normalize: true
        hidden_units: 256
        num_layers: 3

      reward_signals:
        extrinsic:
          gamma: 0.995
          strength: 1.0

      max_steps: 1.2e8     
      time_horizon: 256
      summary_freq: 2000
      checkpoint_interval: 1000000
      threaded: false

  GregorKameraDaleko:
        trainer_type: ppo

        hyperparameters:
          batch_size: 2048
          buffer_size: 40960
          learning_rate: 2.5e-4
          beta: 0.0005
          epsilon: 0.15
          lambd: 0.97
          num_epoch: 4
          learning_rate_schedule: linear

        network_settings:
          normalize: true
          hidden_units: 256
          num_layers: 3

        reward_signals:
          extrinsic:
            gamma: 0.995
            strength: 1.0

        max_steps: 1.2e7     
        time_horizon: 256
        summary_freq: 2000
        checkpoint_interval: 1000000
        threaded: false

  Gregor2.0:
    trainer_type: ppo

    hyperparameters:
      batch_size: 1024
      buffer_size: 20480
      learning_rate: 4.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear

    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple

    reward_signals:
      extrinsic:
        gamma: 0.992
        strength: 1.0

    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 20000
      play_against_current_self_ratio: 0.6

    max_steps: 1.0e7
    time_horizon: 64
    summary_freq: 5000
    checkpoint_interval: 500000
    threaded: false
